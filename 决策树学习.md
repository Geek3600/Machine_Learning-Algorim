# 决策树学习

![屏幕截图 2022-02-22 112738](C:\Users\27410\Desktop\学习\统计学习方法\屏幕截图 2022-02-22 112738.png)

## 学习目的：

​     1. 构造决策树，并对实例正确分类

## 本质：

       1. 从训练数据中归纳出一组最好的**分类规则**，与训练数据不相矛盾

## 假设空间：

1. 由无穷多个条件概率模型组成

## 决策树标准：

1. 与训练数据矛盾较小，且具有较好的泛化能力
2. 深度小(树的高度)：所有节点的最大层次数
3. 叶节点少(宽度)

## 学习策略：

1. 以**损失函数**为目标函数的**最小化**

## 学习算法过程：

涉及：特征选择、决策树的生成、决策树剪枝

### 特征选择：

1. 定义：递归选择最优特征（如果特征过多就要对特征进行选择）

2. 目的：选取对训练数据具有分类能力的特征，提高决策树的学习效率(通过选择合适的特征，剔除了无关紧要的特征，就可以节约学习时间成本）

3. 选择准则：信息增益，信息增益比

   * 熵(H)：表示随机变量不确定性的度量

      ![image-20220222154106995](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222154106995.png)

      熵越大，随机变量的的不确定性就越大

   * 条件熵：表示在已知随机变量X的条件下随机变量Y的不确定性

      ![image-20220222154328932](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222154328932.png)

     

     当**熵**和**条件熵**中的概率是由数据估计得到的时，所对应的熵与条件熵分别称为**经验熵**和**经验条件熵**。

     经验熵：

   * 信息增益 (g)： 表示得知特征X的信息之后而使得类Y的信息不确定性减少的程度。信息增益越大，说明加入特征X之后对不确定性的减小的影响最大，同时也说明加入该特征将会对分类结果的影响越大

   ![image-20220222155240208](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222155240208.png)

   * 信息增益比： 特征A对训练数据集D的信息增益比定义为其信息增益与训练数据集D关于特征A的值的熵之比

     ![image-20220222182529746](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222182529746.png)

     单纯以**信息增益**作为划分数据集的特征时，存在偏向于选择取值较多的特征的问题，而**信息增益比**的效果则相反，所以使用**信息增益比**来校正

### 生成：

1. 对应特征空间的划分，直到所有训练子集被基本分类正确

2. 生成算法：

   * ID3算法：以信息增益为准则

     输入：训练数据集D，特征集A(从D中选择出)， 阈值  d(人为确定)

   * C4.5算法：以信息增益比为准则

### 剪枝：

1. 本质：处理决策树的过拟合问题，提高泛化能力

2. 预剪枝：**在生成过程中**，对每个结点划分前进行估计，若当前节点的划分不能提高泛化能力，则停止划分，记当前节点为叶节点：
   * 限定决策树的高度
   * 设定一个阈值，与生成算法中的阈值相似
   * 设置某个指标，比较结点划分前后的泛化能力
   * 过程：利用**测试集误差率**，对每个结点是否适合划分都进行评估，误差率比上一次小的(即能提高泛化能力的结点)结点就进行划分，否则就进行下一个位置的评估，同时遵循奥卡姆剃刀原理。
   
3. 后剪枝：生成一棵完整的决策树之后，自下而上地对内部结点进行考察，若此内部节点变为叶节点之后可以提升泛化能力，那就进行替换

   * 降低错误剪枝(REP): 自下而上使用测试集，对每个结点计算剪枝前后的误判个数来判断是否需要进行剪枝。(与与预剪枝判断手法相似只是顺序不同)

   * 悲观错误剪枝(PEP): 根据剪枝前后的**错误率**比较来决定是否剪枝。与REP不同的使，PEP只需要训练集即可，不需要验证集，而且是自上而下的

       特点：1.由于只用到训练集，所以不需要分离剪枝数据集，有利于实例较少的问题

     ​             2.误差使用了连续修正值，使得适用性更强

     ​             3.自上而下的剪枝策略	PEP的效率更高

     ​             4.可能会剪掉不应剪掉的枝条

   * 最小误差剪枝(MEP): 根据剪枝前后的**最小分类错误概率**来决定是否剪枝，自下而上，只需要训练集即可
   * 基于错误剪枝(EBP)：根据剪枝前后的**误判个数上界**的大小来决定是否剪枝，自下而上剪枝，只需要训练集即可
   * 代价-复杂度剪枝(CCP)：根据剪枝前后的**损失函数**来决定是否剪枝，损失函数取决于**代价**和**复杂度**

​         各种后剪枝算法和核心思想都是一样的，即通过剪枝前后进行对比来决定是否剪枝，但是每个算法用不同的量来衡量剪枝的价值，他们有各自不同的公式来计算，适用于不同的场景

1. 模型度量：
   * 误差：训练误差，测试误差
   * 过拟合：训练误差很低，测试误差高(模型结构过于复杂)，这是最容易出现的情况
   * 欠拟合：训练误差高，测试误差低(模型结构过于简单)

## 简单案例

如图 ![屏幕截图 2022-02-22 112926](C:\Users\27410\Desktop\学习\统计学习方法\屏幕截图 2022-02-22 112926.png)

