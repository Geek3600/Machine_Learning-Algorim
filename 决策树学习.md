# 决策树学习

![屏幕截图 2022-02-22 112738](C:\Users\27410\Desktop\学习\统计学习方法\屏幕截图 2022-02-22 112738.png)

## 学习目的：

​     1. 构造决策树，并对实例正确分类

## 本质：

1. 从训练数据中归纳出一组最好的**分类规则**，与训练数据不相矛盾

## 假设空间：

1. 由无穷多个条件概率模型组成

## 决策树标准：

1. 与训练数据矛盾较小，且具有较好的泛化能力

## 学习策略：

1. 以**损失函数**为目标函数的**最小化**

## 学习算法过程：

涉及：特征选择、决策树的生成、决策树剪枝

### 特征选择：

1. 定义：递归选择最优特征（如果特征过多就要对特征进行选择）

2. 目的：选取对训练数据具有分类能力的特征，提高决策树的学习效率(通过选择合适的特征，剔除了无关紧要的特征，就可以节约学习时间成本）

3. 选择准则：信息增益，信息增益比

   * 熵(H)：表示随机变量不确定性的度量

      ![image-20220222154106995](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222154106995.png)

      熵越大，随机变量的的不确定性就越大

   * 条件熵：表示在已知随机变量X的条件下随机变量Y的不确定性

      ![image-20220222154328932](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222154328932.png)

     

     当**熵**和**条件熵**中的概率是由数据估计得到的时，所对应的熵与条件熵分别称为**经验熵**和**经验条件熵**。

     经验熵：

   * 信息增益 (g)： 表示得知特征X的信息之后而使得类Y的信息不确定性减少的程度。信息增益越大，说明加入特征X之后对不确定性的减小的影响最大，同时也说明加入该特征将会对分类结果的影响越大

   ![image-20220222155240208](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222155240208.png)

   * 信息增益比： 特征A对训练数据集D的信息增益比定义为其信息增益与训练数据集D关于特征A的值的熵之比

     ![image-20220222182529746](C:\Users\27410\AppData\Roaming\Typora\typora-user-images\image-20220222182529746.png)

     单纯以**信息增益**作为划分数据集的特征时，存在偏向于选择取值较多的特征的问题，而**信息增益比**的效果则相反，所以使用**信息增益比**来校正

### 生成：

1. 对应特征空间的划分，直到所有训练子集被基本分类正确

3. 剪枝：对已生成的树，自上而下进行剪枝，讲述变得更简单，避免过拟合，具有更好的泛化能力（类似于园艺工人对花草进行修剪，将花草的多余枝叶去除）

   

## 简单案例

如图 ![屏幕截图 2022-02-22 112926](C:\Users\27410\Desktop\学习\统计学习方法\屏幕截图 2022-02-22 112926.png)

